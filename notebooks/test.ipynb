{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'compiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcjm_pytorch_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pil_to_tensor, tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor, move_data_to_device\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcjm_pandas_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m markdown_to_pandas, convert_to_numeric, convert_to_string\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcjm_torchvision_tfms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResizeMax, PadSquare, CustomRandomIoUCrop\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Import the distinctipy module\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistinctipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distinctipy\n",
      "File \u001b[0;32m/media/james/2b44c141-eec6-4c63-a888-30e9ac5660bd/git/maskrcnn_from_scratch/.venv/lib/python3.10/site-packages/cjm_torchvision_tfms/core.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     23\u001b[0m torchvision\u001b[38;5;241m.\u001b[39mdisable_beta_transforms_warning()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtv_tensors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoundingBoxes, Mask\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tv_tensors\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_bounding_boxes\n",
      "File \u001b[0;32m/media/james/2b44c141-eec6-4c63-a888-30e9ac5660bd/git/maskrcnn_from_scratch/.venv/lib/python3.10/site-packages/torchvision/tv_tensors/__init__.py:14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_video\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Video\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# TODO: Fix this. We skip this method as it leads to\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# RecursionError: maximum recursion depth exceeded while calling a Python object\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Until `disable` is removed, there will be graph breaks after all calls to functional transforms\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241m.\u001b[39mdisable\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(wrappee, \u001b[38;5;241m*\u001b[39m, like, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a :class:`torch.Tensor` (``wrappee``) into the same :class:`~torchvision.tv_tensors.TVTensor` subclass as ``like``.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    If ``like`` is a :class:`~torchvision.tv_tensors.BoundingBoxes`, the ``format`` and ``canvas_size`` of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m            Ignored otherwise.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(like, BoundingBoxes):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'compiler'"
     ]
    }
   ],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "import datetime\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_psl_utils.core import download_file, file_extract, get_source_code\n",
    "from cjm_pil_utils.core import resize_img, get_img_files, stack_imgs\n",
    "from cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor, move_data_to_device\n",
    "from cjm_pandas_utils.core import markdown_to_pandas, convert_to_numeric, convert_to_string\n",
    "from cjm_torchvision_tfms.core import ResizeMax, PadSquare, CustomRandomIoUCrop\n",
    "\n",
    "# Import the distinctipy module\n",
    "from distinctipy import distinctipy\n",
    "\n",
    "# Import matplotlib for creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Set options for Pandas DataFrame display\n",
    "pd.set_option('max_colwidth', None)  # Do not truncate the contents of cells in the DataFrame\n",
    "pd.set_option('display.max_rows', None)  # Display all rows in the DataFrame\n",
    "pd.set_option('display.max_columns', None)  # Display all columns in the DataFrame\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtnt.utils import get_module_summary\n",
    "import torchvision\n",
    "from torchvision.models.detection.image_list import BoundingBoxes, Mask\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "import torchvision.transforms.v2  as transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "# Import Mask R-CNN\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
