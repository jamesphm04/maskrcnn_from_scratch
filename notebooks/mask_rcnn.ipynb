{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import torch \n",
    "import os \n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T \n",
    "import cv2 \n",
    "import os \n",
    "from typing import Literal, Union\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['GLOBALSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes: int):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256 #should be 256\n",
    "    \n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model \n",
    "\n",
    "model = get_model(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = [   \n",
    "    # A.Resize(256, 256, p=1),  \n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_transforms = [\n",
    "    # A.Resize(256, 256, p=1), \n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data/sofia_data/ground_truth_images'\n",
    "images_boxes_dir = 'data/sofia_data/ground_truth_bounding_boxes'\n",
    "images_ellipses_dir = 'data/sofia_data/ground_truth_projected_ellipses'\n",
    "\n",
    "image_dir_items = [i.split(\".\")[0] for i in os.listdir(image_dir)]\n",
    "images_ellipses_dir_items = [i.split(\".\")[0] for i in os.listdir(images_ellipses_dir)]\n",
    "items = list(set(image_dir_items) & set(images_ellipses_dir_items))\n",
    "items = sorted(items, key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "images_name = [i+'.png' for i in items]\n",
    "ellipses_name = [i+'.txt' for i in items]\n",
    "bboxes_name = [i+'.txt' for i in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CraterDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 stage: Literal[\"train\", \"val\", 'test'],\n",
    "                 transforms: Union[A.Compose, T.Compose] = None,\n",
    "                 ):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.imgs_dir = image_dir\n",
    "        self.images_ellipses_dir = images_ellipses_dir\n",
    "        \n",
    "        if stage == 'train':\n",
    "            self.imgs = images_name[:800]  \n",
    "            \n",
    "        elif stage == 'val': \n",
    "            self.imgs = images_name[800:1000]\n",
    "        else: \n",
    "            self.imgs = images_name[1000:1200]\n",
    "        \n",
    "        if transforms:\n",
    "            self.bbox_params = {\n",
    "                \"format\":\"pascal_voc\",\n",
    "                \"min_area\": 0,\n",
    "                \"min_visibility\": 0,\n",
    "                \"label_fields\": [\"category_id\"]\n",
    "            }\n",
    "            self.transforms = A.Compose(transforms, bbox_params=self.bbox_params)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, dict]:\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.imgs_dir, img_name)\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        \n",
    "        # annotation file\n",
    "        annot_filename = img_name[:-4] + '.txt'\n",
    "        annot_file_path = os.path.join(self.images_ellipses_dir, annot_filename)\n",
    "        \n",
    "        target = self._get_target(annot_file_path)\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.transforms:\n",
    "            img_rgb, target = self.transform(img_rgb, target)\n",
    "        return img_rgb, target\n",
    "    \n",
    "    def transform(self, image: np.ndarray, target: dict) -> tuple[torch.Tensor, dict]:\n",
    "        transformed = self.transforms(\n",
    "            image=image, masks=target[\"masks\"],\n",
    "            bboxes=target[\"boxes\"], \n",
    "            category_id=target[\"labels\"]\n",
    "        )\n",
    "    \n",
    "        image = transformed[\"image\"]\n",
    "        target[\"masks\"] = torch.as_tensor(\n",
    "            np.array(list(map(np.array, transformed[\"masks\"])), dtype=np.uint8)\n",
    "        ) \n",
    "        \n",
    "        target[\"labels\"] = torch.tensor(transformed[\"category_id\"])\n",
    "        target[\"boxes\"] = torch.as_tensor(transformed[\"bboxes\"], dtype=torch.float32)\n",
    "        target[\"area\"] = self.__get_area(target[\"boxes\"])\n",
    "        return image, target\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_target_sample() -> dict:\n",
    "        return {\n",
    "            \"boxes\": [],\n",
    "            \"masks\": [],\n",
    "            \"area\": [],\n",
    "            \"labels\": [],\n",
    "            \"iscrowd\": None,\n",
    "            \"image_id\": None\n",
    "        }\n",
    "\n",
    "    def _get_target(self, annotations_path: str) -> dict:\n",
    "        target = self._get_target_sample()\n",
    "\n",
    "        with open(annotations_path, \"r\") as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                label = 1 \n",
    "                data = line.strip().split(',')\n",
    "                # Extract ellipse parameters\n",
    "                x_centre, y_centre, semi_major_axis, semi_minor_axis, rotation = map(float, data)\n",
    "                rotation = np.degrees(rotation)\n",
    "                mask = self.__get_mask(x_centre, y_centre, semi_major_axis, semi_minor_axis, rotation)\n",
    "                box = self.__get_box(mask)\n",
    "                target[\"masks\"].append(mask)    \n",
    "                target[\"boxes\"].append(box)\n",
    "                target[\"labels\"].append(label)\n",
    "                \n",
    "        num_objs = len(target[\"labels\"])\n",
    "        target[\"iscrowd\"] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        return target \n",
    "\n",
    "    @staticmethod\n",
    "    def __get_mask( x_centre, y_centre, semi_major_axis, semi_minor_axis, rotation) -> np.ndarray:\n",
    "        mask = np.zeros((1024, 1024), dtype=np.uint8)\n",
    "     \n",
    "        cv2.ellipse(mask, (int(x_centre), int(y_centre)),\n",
    "                    (int(semi_major_axis), int(semi_minor_axis)),\n",
    "                    angle=rotation, startAngle=0, endAngle=360,\n",
    "                    color=1, thickness=-1)\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_box(mask: np.ndarray):\n",
    "        pos = np.nonzero(mask)\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_area(boxes) -> torch.Tensor:\n",
    "         return (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CraterDataset(\n",
    "    stage=\"train\",\n",
    "    transforms=train_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CraterDataset(\n",
    "    stage=\"val\",\n",
    "    transforms=val_test_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=6, \n",
    "    shuffle=True,\n",
    "    # num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    # num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 val_dataloader: DataLoader,\n",
    "                 early_stop: dict = {\"monitor\": \"loss_mask\", \"patience\": 5},\n",
    "                 save_every_epoch: int = 1,\n",
    "                 save_dirpath: str = 'runs'\n",
    "                ):\n",
    "        \n",
    "        # Callbacks | Early stoping & Model checkpoint\n",
    "        self.patience = early_stop[\"patience\"]\n",
    "        self.monitor = early_stop[\"monitor\"]\n",
    "        self.track_list = []\n",
    "        self.save_every_epoch = save_every_epoch\n",
    "        self.save_dirpath = save_dirpath\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.train_batches = len(train_dataloader)\n",
    "        self.val_batches =  len(val_dataloader)\n",
    "        \n",
    "        self.model = model\n",
    "        self.setup_model()\n",
    "        \n",
    "        self.optim_dict = self.configure_optimizers()\n",
    "        self.optimizer = self.optim_dict[\"optimizer\"]\n",
    "        self.lr_scheduler = self.optim_dict[\"lr_scheduler\"]\n",
    "        \n",
    "        self.step_outputs = {\n",
    "            \"loss_objectness\": 0,\n",
    "            \"loss_mask\": 0,\n",
    "            \"loss_classifier\": 0,\n",
    "            \"loss_rpn_box_reg\": 0,\n",
    "            \"loss_box_reg\": 0,\n",
    "            \"loss\": 0\n",
    "        }\n",
    "        \n",
    "    def configure_optimizers(self) -> dict:\n",
    "        # construct an optimizer\n",
    "        params = [\n",
    "            p\n",
    "            for p in self.model.parameters()\n",
    "            if p.requires_grad\n",
    "        ]\n",
    "\n",
    "        optimizer = optim.SGD(\n",
    "            params,\n",
    "            lr=0.0018,\n",
    "            momentum=0.938,\n",
    "            weight_decay=0.00053\n",
    "        )\n",
    "\n",
    "        # and a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=3,\n",
    "            gamma=0.1\n",
    "        )\n",
    "        return {\n",
    "            \"lr_scheduler\": lr_scheduler,\n",
    "            \"optimizer\": optimizer\n",
    "        }\n",
    "    \n",
    "    def setup_model(self) -> None:\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "    \n",
    "    def to_device(self, batch: tuple) -> tuple:\n",
    "        images, targets = batch\n",
    "        images = list(image.to(self.device) for image in images)\n",
    "\n",
    "        targets = [\n",
    "            {key: value.to(self.device) \n",
    "             for key, value in target.items()}\n",
    "            for target in targets\n",
    "        ]\n",
    "        \n",
    "        return images, targets\n",
    "    \n",
    "    def training_step(self, batch) -> dict:\n",
    "        images, targets = self.to_device(batch)\n",
    "        self.optimizer.zero_grad() \n",
    "        outputs = self.model(images, targets)\n",
    "        loss = sum([loss for loss in outputs.values()])\n",
    "        outputs[\"loss\"] = loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.lr_scheduler.step()\n",
    "        return outputs\n",
    "    \n",
    "    def validation_step(self, batch) -> dict:\n",
    "        images, targets = self.to_device(batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(images, targets)\n",
    "            loss = sum([loss for loss in outputs.values()])\n",
    "            outputs[\"loss\"] = loss\n",
    "        return outputs\n",
    "    \n",
    "    def shared_epoch_end(self, stage: str, epoch: int) -> float:\n",
    "        tracked_loss = self.step_outputs[self.monitor]    \n",
    "        loss_objectness = self.step_outputs[\"loss_objectness\"]\n",
    "        loss_mask = self.step_outputs[\"loss_mask\"]\n",
    "        loss_classifier = self.step_outputs[\"loss_classifier\"]\n",
    "        loss_rpn_box_reg = self.step_outputs[\"loss_rpn_box_reg\"]\n",
    "        loss_box_reg = self.step_outputs[\"loss_box_reg\"]\n",
    "        loss = self.step_outputs[\"loss\"]\n",
    "        \n",
    "        print(\n",
    "            f\"\"\"\n",
    "            || End {epoch} {stage} epoch ||\n",
    "            loss_objectness: {loss_objectness:.2f}\n",
    "            loss_mask: {loss_mask:.2f}\n",
    "            loss_classifier: {loss_classifier:.2f}\n",
    "            loss_rpn_box_reg: {loss_rpn_box_reg:.2f}\n",
    "            loss_box_reg: {loss_box_reg:.2f} \n",
    "            loss: {loss:.2f}\\n\n",
    "            \"\"\"\n",
    "        )\n",
    "              \n",
    "        self.step_outputs = self.step_outputs.fromkeys(self.step_outputs, 0)\n",
    "        if stage == \"val\":\n",
    "            return tracked_loss\n",
    "              \n",
    "    def on_train_epoch_end(self, epoch: int) -> None:\n",
    "        return self.shared_epoch_end(stage=\"train\", epoch=epoch)\n",
    "\n",
    "    def on_validation_epoch_end(self, epoch: int) -> None:\n",
    "        tracked_loss = self.shared_epoch_end(stage=\"val\", epoch=epoch)\n",
    "        patience = 0\n",
    "        \n",
    "        if epoch > self.patience:\n",
    "            last_tracked = list(reversed(self.track_list))[:self.patience]\n",
    "            for i in last_tracked:\n",
    "                if i <= tracked_loss:\n",
    "                    patience += 1\n",
    "                    \n",
    "        self.track_list.append(tracked_loss)\n",
    "        return tracked_loss, (self.patience - patience)\n",
    "\n",
    "    def train(self, max_epochs: int) -> None:\n",
    "        \n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            for batch_idx, batch in tqdm(enumerate(self.train_dataloader, 1), desc=\"Training\", total=self.train_batches, colour=\"#068e58\"):\n",
    "                outputs = self.training_step(batch)\n",
    "                for key, value in outputs.items():\n",
    "                    self.step_outputs[key] += float(value.detach().cpu().numpy()) / self.train_batches\n",
    "            self.on_train_epoch_end(epoch)\n",
    "\n",
    "            for batch_idx, batch in tqdm(enumerate(val_dataloader, 1), desc=\"Validation\", total=self.val_batches, colour=\"#013385\"):\n",
    "                outputs = self.validation_step(batch)\n",
    "                for key, value in outputs.items():\n",
    "                    self.step_outputs[key] += float(value.detach().cpu().numpy()) / self.val_batches\n",
    "            tracked_loss, patience = self.on_validation_epoch_end(epoch)\n",
    "            \n",
    "            if epoch % self.save_every_epoch == 0:\n",
    "                if not os.path.exists(self.save_dirpath):\n",
    "                    os.mkdir(self.save_dirpath)\n",
    "                path = os.path.join(self.save_dirpath, f\"epoch_{epoch}_{self.monitor}_{tracked_loss:.2f}.pt\")\n",
    "                torch.save(model.state_dict(), path) \n",
    "                print(\"\\nThe model passed the save checkpoint successfully!\\n\")\n",
    "                \n",
    "            if patience == 0:\n",
    "                print(\"Our patience has run out! Model training stopped beforehand.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    early_stop = {\"monitor\": \"loss_mask\", \"patience\": 5},\n",
    "    save_every_epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|\u001b[38;2;6;142;88m          \u001b[0m| 0/134 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('path/to/model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = CraterDataset(stage=\"test\",transforms=val_test_transforms)\n",
    "test_image_names = images_name[1000:1200]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for img_idx in range(20):\n",
    "    img, target = test_dataset[img_idx]\n",
    "    real_img_idx = test_image_names[img_idx][:-4]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = img.to(device)  # Move input tensor to the same device as the model\n",
    "        prediction = model([img])[0]\n",
    "\n",
    "    print('predicted #boxes: ', len(prediction['labels']))\n",
    "    print('real #boxes: ', len(target['labels']))\n",
    "\n",
    "    target['image_id']\n",
    "\n",
    "    \n",
    "    test_image = cv2.imread(f'./data/sofia_data/ground_truth_images/{real_img_idx}.png')\n",
    "    # test_image_scaled = cv2.resize(test_image, (256, 256))\n",
    "    for box in prediction['boxes']:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        # area = (x2 - x1) * (y2 - y1)\n",
    "        \n",
    "        cv2.rectangle(test_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 1)\n",
    "\n",
    "    images_boxes_path = os.path.join(images_boxes_dir, f'{real_img_idx}.txt')\n",
    "\n",
    "    with open(images_boxes_path, 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in lines:\n",
    "            data = line.strip().split(',')\n",
    "            # Extract box parameters\n",
    "            xmin, ymin, xmax, ymax = map(float, data)\n",
    "            \n",
    "            cv2.rectangle(test_image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 1)\n",
    "\n",
    "    #####\n",
    "\n",
    "    #####\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
